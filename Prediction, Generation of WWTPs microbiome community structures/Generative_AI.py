import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torch.utils import dataimport numpy as npimport matplotlib.pyplot as pltfrom sklearn.preprocessing import MinMaxScalerimport torchvisionfrom torchvision import transformsfrom load_data import load_dataimport csvclass Generator(nn.Module):    def __init__(self):        super(Generator, self).__init__() # 继承父类        self.main = nn.Sequential(            nn.Linear(2, 4), nn.ReLU(),            nn.Linear(4, 8), nn.ReLU(),            nn.Linear(8, 16), nn.ReLU(),            nn.Linear(16, 32), nn.ReLU(),            nn.Linear(32, 58), nn.ReLU(),            nn.Linear(58, 58*1))            # nn.Linear(100, 256), nn.ReLU(),            # nn.Linear(256, 512), nn.ReLU(),            # nn.Linear(512, 28*28),            # nn.Tanh()) # 最后必须用tanh，把数据分布到（-1，1）之间            # nn.Sigmoid())    def forward(self, x):  # x表示长度为100的噪声输入        img = self.main(x)        img = img.view(-1, 58)  # 方便等会绘图        # img = img.view(-1, 28, 28, 1) # 方便等会绘图        return imgclass Discriminator(nn.Module):    def __init__(self):        super(Discriminator,self).__init__()        self.main = nn.Sequential(            nn.Linear(58 * 1, 58),            nn.LeakyReLU(),  # x小于零是是一个很小的值不是0，x大于0是还是x            nn.Linear(58, 32),            nn.LeakyReLU(),            nn.Linear(32, 16),            nn.LeakyReLU(),            nn.Linear(16, 8),            nn.LeakyReLU(),            nn.Linear(8, 4),            nn.LeakyReLU(),            nn.Linear(4, 2),            nn.LeakyReLU(),            nn.Linear(2, 1),            # nn.Linear(28*28, 512),            # nn.LeakyReLU(), # x小于零是是一个很小的值不是0，x大于0是还是x            # nn.Linear(512, 256),            # nn.LeakyReLU(),            # nn.Linear(256, 1),            nn.Sigmoid() # 保证输出范围为（0，1）的概率        )    def forward(self, x): # x表示28*28的mnist图片        img = x.view(-1, 58)        img= torch.tensor(img, dtype=torch.float32).cuda(device)        img = self.main(img)        return imgdef gen_img_plot(model, test_input):    prediction = np.squeeze(model(test_input).detach().cpu().numpy())    fig = plt.figure(figsize=(25, 25))    for i in range(1000):        plt.subplot(25, 4, i+1) # 四行四列的第一个        # imshow函数绘图的输入是（0，1）的float，或者（1，256）的int        # 但prediction是tanh出来的范围是[-1，1]没法绘图，需要转成0~1(即加1除2)。        plt.imshow( (prediction[i]+1)/2 )        plt.axis('off')    plt.show()device = 'cuda' if torch.cuda.is_available() else 'cpu'print('training on ', device)# 模型gen = Generator().to(device)dis = Discriminator().to(device)# 优化器g_opt = torch.optim.Adam(gen.parameters(), lr=0.0001)d_opt = torch.optim.Adam(dis.parameters(), lr=0.0001)# 损失loss = torch.nn.BCELoss()# test_input = torch.randn(100, 16, device=device)   #100个噪音，输入16维transform = transforms.Compose([    transforms.ToTensor(),  # 归一化为0~1    transforms.Normalize(0.5,0.5) # 归一化为-1~1])# train_ds = torchvision.datasets.MNIST('datasets',  # 下载到那个目录下#                                       train=True,#                                       transform=transform,#                                       download=True)def Generative_AI(file_name):# file_name = 'D://BaiduNetdiskDownload//Sample_information_final-Phylums.csv'    Df, data_set, M = load_data(file_name)    # train_ds = data_set[:int(M * 0.8), :]    # train_ds = np.array(train_ds, dtype='uint32')    # train_ds_tensor = transform(train_ds)    # train_ds = train_ds_tensor[-1].cpu().detach().numpy()    scale = MinMaxScaler()    # train_ds = data_set[:int(M * 0.8), :]    train_ds = scale.fit_transform(data_set[:int(M * 1), :])    train_ds_tensor = torch.tensor(train_ds)    dataloader = torch.utils.data.DataLoader(train_ds_tensor, batch_size=100, shuffle=True)  #训练数据集每100个放入神经网络    imgs = next(iter(dataloader))    A = imgs.shape    # torch.Size([64, 1, 28, 28])    D_loss = []    G_loss = []    Gen_img_tensor = []    fake_output = []    epochs = 1500    for epoch in range(epochs):        d_epoch_loss = 0        g_epoch_loss = 0        gen_img = 0        count = len(dataloader)  # 一个epoch的大小        for step, (img) in enumerate(dataloader):            img = img.to(device)  # 一个批次的图片            size = img.size(0)  # 和图片对应的原始噪音            # random_noise = abs(torch.randn(size, 16, device=device))            random_noise = torch.randint(0, 100000, [size, 2], device=device).float()            random_noise = random_noise/100000            noise = random_noise.cpu().detach().numpy()            gen_img = gen(random_noise)   # 生成的图像            gen_img = abs(gen_img)            Gen_img_tensor.append(gen_img)            d_opt.zero_grad()            real_output = dis(img)  # 判别器输入真实图片，对真实图片的预测结果，希望是1            # 判别器在真实图像上的损失            d_real_loss = loss(real_output, torch.ones_like(real_output))  # size一样全一的tensor            d_real_loss.backward()            g_opt.zero_grad()            # 记得切断生成器的梯度            fake_output = dis(gen_img)#.detach())  # 判别器输入生成图片，对生成图片的预测结果，希望是0            # 判别器在生成图像上的损失            d_fake_loss = loss(fake_output, torch.zeros_like(fake_output))  # size一样全一的tensor            d_fake_loss.backward()            d_loss = d_real_loss + d_fake_loss            d_opt.step()            # 生成器的损失            g_opt.zero_grad()            fake_output = dis(gen_img)  # 希望被判定为1            g_loss = loss(fake_output, torch.ones_like(fake_output))            g_loss.backward()            g_opt.step()            # 每个epoch内的loss累加，循环外再除epoch大小，得到平均loss            with torch.no_grad():                d_epoch_loss += d_loss                g_epoch_loss += g_loss        # 一个epoch训练完成        with torch.no_grad():            Gen_imgs = []            d_epoch_loss /= count            g_epoch_loss /= count            D_loss.append(d_epoch_loss)            G_loss.append(g_epoch_loss)            for i in range(10):                Gen_img = Gen_img_tensor[i].cpu().detach().numpy()                Gen_imgs.append(Gen_img)            Gen_img = np.concatenate(Gen_imgs, axis=0)        #       writer.writerows(Gen_img)            # Gen_img_tensor.append(gen_img)            print('Generated Epoch: ', epoch, 'epoch')            # print('Gen_img: ', Gen_img)    # 1️⃣ 读取源 CSV 文件的表头    with open('D://BaiduNetdiskDownload//Sample_information_final-Phylums.csv', 'r', encoding='utf-8') as f_in:        reader = csv.reader(f_in)        headers = next(reader)  # 获取第一行（表头）    # 2️⃣ 写入目标 CSV 文件    # with open('D://BaiduNetdiskDownload//Generated_data_Phylums.csv', 'w', encoding='utf-8', newline='') as f_out:    #     writer = csv.writer(f_out)    #     writer.writerow(headers)  # 写入表头    #     for i in range(10):    #         Gen_img = Gen_img_tensor[i].cpu().detach().numpy()    #         writer.writerows(Gen_img)    # print('The CSV file is generated!')    # print(len(Gen_img_tensor))    return headers, Gen_img, len(Gen_img)# with open("E://BaiduNetdiskDownload//Generated_data_Phylums.csv", 'w', encoding='utf-8',) as csvfile:#     writer = csv.writer(csvfile, lineterminator='\n')#     # first write columns_name#     writer.writerow(['Annual average(℃)', 'Annual mean of daily maximum(℃)',#        'Annual mean of daily minimum(℃)', 'Sampling month average(℃)',#        'Sampling moment(℃)', 'Annual(mm)', 'Sampling month(mm)',#        'GDP per capita (dollars)', 'City population', 'Actual Inf rate (m3/d)',#        'HRT (hr) Plant', 'HRT (hr) Aeration tank', 'SRT (d)', 'BOD:(mg/l):Inf',#        'BOD:inf/(1+recycle ratio)', 'BOD:Aeration tank inf',#        'BOD-Removal rate', 'COD:(mg/l):Inf', 'COD:Inf/(1+recycle ratio)',#        'COD:Aeration tank inf', 'COD-Removal rate', 'NH4-N:(mg/l):Inf',#        'NH4-N:Aeration tank inf', 'NH4-Removal rate', 'TN:(mg/l):Inf',#        'TN:Aeration tank inf', 'TN-Removal rate', 'TP:(mg/l):Inf',#        'TP:Aeration tank inf', 'TP-Removal rate', 'Percentage', 'MLSS (mg/l)',#        'DO (mg/l)', 'pH', 'Mixed liquid temperature(℃)',#        'Conductivity (μS/cm)', 'SVI (ml/g)', 'Proteobacteria', 'Bacteroidota',#        'Patescibacteria', 'Unclassified', 'Firmicutes', 'Planctomycetota',#        'Verrucomicrobiota', 'Chloroflexi', 'Myxococcota', 'Others',#        'Bdellovibrionota', 'Actinobacteriota', 'Acidobacteriota',#        'Desulfobacterota', 'Dependentiae', 'Cyanobacteria', 'Spirochaetota',#        'Nanoarchaeota', 'Elusimicrobiota', 'SAR324 clade(Marine group B)',#        'Armatimonadota'])#     # then write data#     writer.writerows(Gen_img)#     print('The CSV file is generated!')